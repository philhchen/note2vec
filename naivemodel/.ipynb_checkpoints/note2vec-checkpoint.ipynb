{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test', 'train', 'valid'])\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "with open('/Users/Phil/Documents/Frosh/CS224N/note2vec/data/jsb-chorales-16th.pkl', 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    p = u.load()\n",
    "    \n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52 unique notes\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for dataset in data:\n",
    "    for chorale in data[dataset]:\n",
    "        for chord in chorale:\n",
    "            vocab = vocab.union(chord)\n",
    "w2i = {w: i for i, w in enumerate(vocab)}\n",
    "i2w = {i: w for i, w in enumerate(vocab)}\n",
    "notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "\n",
    "def num2note(num):\n",
    "    note_name = notes[num % 12]\n",
    "    octave = str((num-12)//12)\n",
    "    return note_name + octave\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print('Found', vocab_size, 'unique notes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random\n",
    "\n",
    "counts = {w2i[note]:1 for note in vocab}\n",
    "for dataset in data:\n",
    "    for chorale in data[dataset]:\n",
    "        for chord in chorale:\n",
    "            for note in chord:\n",
    "                counts[w2i[note]] += 1\n",
    "\n",
    "total = sum([counts[k] for k in counts])\n",
    "vals = [counts[k]/total for k in counts]\n",
    "            \n",
    "def getNegativeSample():\n",
    "    return numpy.random.choice(list(counts.keys()), p=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "def create_skipgram_dataset(chorales):\n",
    "    data = []\n",
    "    for ch in chorales:\n",
    "        for c in ch:\n",
    "            for n in c:\n",
    "                data += [(w2i[n], w2i[n1], 1) for n1 in c if n1 != n]\n",
    "                # negative sample\n",
    "                data += [(w2i[n], getNegativeSample(), 0) for _ in range(3)]\n",
    "    dataset = TensorDataset(torch.tensor(skipgram_data, dtype=torch.long))\n",
    "    loader = DataLoader(sg_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    return loader\n",
    "\n",
    "sg_loader = create_skipgram_dataset(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus)\n",
    "        embed_focus = embed_focus.view((embed_focus.shape[0], 1, embed_focus.shape[1]))\n",
    "        embed_context = self.embeddings(context)\n",
    "        embed_context = embed_context.view((embed_context.shape[0], embed_context.shape[1], 1))\n",
    "        scores = torch.bmm(embed_focus, embed_context)\n",
    "        log_probs = F.logsigmoid(scores)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram(\n",
      "  (embeddings): Embedding(52, 8)\n",
      ")\n",
      "Epoch: 0 Loss: 9377.3212890625\n",
      "Epoch: 1 Loss: 5754.92919921875\n",
      "Epoch: 2 Loss: 5353.03271484375\n",
      "Epoch: 3 Loss: 5214.1640625\n",
      "Epoch: 4 Loss: 5136.5703125\n",
      "Epoch: 5 Loss: 5083.423828125\n",
      "Epoch: 6 Loss: 5045.63134765625\n",
      "Epoch: 7 Loss: 5019.3740234375\n",
      "Epoch: 8 Loss: 5001.21728515625\n",
      "Epoch: 9 Loss: 4988.39599609375\n",
      "Epoch: 10 Loss: 4979.05029296875\n",
      "Epoch: 11 Loss: 4972.03515625\n",
      "Epoch: 12 Loss: 4966.55859375\n",
      "Epoch: 13 Loss: 4962.13427734375\n",
      "Epoch: 14 Loss: 4958.53515625\n",
      "Epoch: 15 Loss: 4955.49658203125\n",
      "Epoch: 16 Loss: 4952.90087890625\n",
      "Epoch: 17 Loss: 4950.65771484375\n",
      "Epoch: 18 Loss: 4948.7119140625\n",
      "Epoch: 19 Loss: 4946.97607421875\n",
      "Epoch: 20 Loss: 4945.44140625\n",
      "Epoch: 21 Loss: 4944.07275390625\n",
      "Epoch: 22 Loss: 4942.82177734375\n",
      "Epoch: 23 Loss: 4941.6923828125\n",
      "Epoch: 24 Loss: 4940.673828125\n",
      "Epoch: 25 Loss: 4939.73779296875\n",
      "Epoch: 26 Loss: 4938.87744140625\n",
      "Epoch: 27 Loss: 4938.07275390625\n",
      "Epoch: 28 Loss: 4937.34375\n",
      "Epoch: 29 Loss: 4936.66552734375\n"
     ]
    }
   ],
   "source": [
    "embed_size = 8\n",
    "learning_rate = 0.01\n",
    "n_epoch = 20\n",
    "\n",
    "def train_skipgram():\n",
    "    losses = []\n",
    "    loss_fn = nn.L1Loss()\n",
    "    model = SkipGram(vocab_size, embed_size)\n",
    "    print(model)\n",
    "    # model.load_state_dict(torch.load('model'))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss = 0.0\n",
    "        for i, sample_batched in enumerate(sg_loader):\n",
    "            sample_batched = sample_batched[0]\n",
    "            in_w_var = Variable(sample_batched[:,0])\n",
    "            ctx_w_var = Variable(sample_batched[:,1])\n",
    "            # print(in_w_var.shape)\n",
    "            model.zero_grad()\n",
    "            log_probs = model(in_w_var, ctx_w_var)\n",
    "            loss = loss_fn(log_probs, Variable(sample_batched[:,2].float()))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data\n",
    "    \n",
    "        losses.append(total_loss.item())\n",
    "        print('Epoch:', epoch, 'Loss:', total_loss.item())\n",
    "        torch.save(model.state_dict(), 'model8')\n",
    "    return model, losses\n",
    "\n",
    "sg_model, sg_losses = train_skipgram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from sklearn.decomposition import PCA\n",
    "model = SkipGram(vocab_size, embed_size)\n",
    "model.load_state_dict(torch.load('model{}'.format(embed_size)))\n",
    "\n",
    "embeddings = np.array(model.embeddings.weight.data)\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(embeddings)\n",
    "\n",
    "with open('embeddings{}.txt'.format(embed_size), 'w') as f:\n",
    "    for i in range(len(embeddings)):\n",
    "        f.write(\"{}\\t{}\\t{}\\t{}\\n\".format(embeddings[i,0], embeddings[i,1], embeddings[i,2], embeddings[i,3]))\n",
    "\n",
    "with open('meta.tsv', 'w') as f:\n",
    "    for i in range(len(i2w)):\n",
    "        f.write('{}\\n'.format(num2note(i2w[i])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
